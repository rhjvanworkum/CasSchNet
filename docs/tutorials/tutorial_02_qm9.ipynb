{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a neural network on QM9\n",
    "\n",
    "This tutorial will explain how to use SchNetPack for training a model\n",
    "on the QM9 dataset and how the trained model can be used for further applications.\n",
    "\n",
    "First, we import the necessary modules and create a new directory for the data and our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import schnetpack as spk\n",
    "from schnetpack.datasets import QM9\n",
    "import schnetpack.transform as trn\n",
    "from ase.db import connect\n",
    "\n",
    "import torch\n",
    "import torchmetrics\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "qm9tut = './qm9tut'\n",
    "if not os.path.exists('qm9tut'):\n",
    "    os.makedirs(qm9tut)\n",
    "\n",
    "with connect('./qm9.db') as conn:\n",
    "  conn.metadata = {\"_distance_unit\": 'nm',\n",
    "                   \"_property_unit_dict\": {\"energy_U0\": 1.0},\n",
    "                  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "As explained in the [previous tutorial](tutorial_01_preparing_data.ipynb), datasets in SchNetPack are loaded with the `AtomsLoader` class or one of the sub-classes that are specialized for common benchmark datasets. \n",
    "The `QM9` dataset class will download and convert the data. We will only use the inner energy at 0K `U0`, so all other properties do not need to be loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Split file was given, but `num_train (107108) != len(train_idx)` (0)!\n",
      "WARNING:root:Split file was given, but `num_val (26777) != len(val_idx)` (0)!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'atomrefs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\rhjva\\imperial\\schnetpack\\docs\\tutorials\\tutorial_02_qm9.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rhjva/imperial/schnetpack/docs/tutorials/tutorial_02_qm9.ipynb#ch0000003?line=0'>1</a>\u001b[0m qm9data \u001b[39m=\u001b[39m QM9(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rhjva/imperial/schnetpack/docs/tutorials/tutorial_02_qm9.ipynb#ch0000003?line=1'>2</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m./qm9.db\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/rhjva/imperial/schnetpack/docs/tutorials/tutorial_02_qm9.ipynb#ch0000003?line=2'>3</a>\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rhjva/imperial/schnetpack/docs/tutorials/tutorial_02_qm9.ipynb#ch0000003?line=14'>15</a>\u001b[0m     load_properties\u001b[39m=\u001b[39m[QM9\u001b[39m.\u001b[39mU0], \u001b[39m#only load U0 property,\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rhjva/imperial/schnetpack/docs/tutorials/tutorial_02_qm9.ipynb#ch0000003?line=15'>16</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/rhjva/imperial/schnetpack/docs/tutorials/tutorial_02_qm9.ipynb#ch0000003?line=16'>17</a>\u001b[0m qm9data\u001b[39m.\u001b[39mprepare_data()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/rhjva/imperial/schnetpack/docs/tutorials/tutorial_02_qm9.ipynb#ch0000003?line=17'>18</a>\u001b[0m qm9data\u001b[39m.\u001b[39;49msetup()\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\schnetpack\\lib\\site-packages\\schnetpack\\data\\datamodule.py:188\u001b[0m, in \u001b[0;36mAtomsDataModule.setup\u001b[1;34m(self, stage)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/schnetpack/lib/site-packages/schnetpack/data/datamodule.py?line=185'>186</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_val_dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39msubset(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_idx)\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/schnetpack/lib/site-packages/schnetpack/data/datamodule.py?line=186'>187</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_test_dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39msubset(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_idx)\n\u001b[1;32m--> <a href='file:///d%3A/Anaconda3/envs/schnetpack/lib/site-packages/schnetpack/data/datamodule.py?line=187'>188</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msetup_transforms()\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\schnetpack\\lib\\site-packages\\schnetpack\\data\\datamodule.py:301\u001b[0m, in \u001b[0;36mAtomsDataModule.setup_transforms\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/schnetpack/lib/site-packages/schnetpack/data/datamodule.py?line=297'>298</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msetup_transforms\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/schnetpack/lib/site-packages/schnetpack/data/datamodule.py?line=298'>299</a>\u001b[0m     \u001b[39m# setup transforms\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/schnetpack/lib/site-packages/schnetpack/data/datamodule.py?line=299'>300</a>\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_transforms:\n\u001b[1;32m--> <a href='file:///d%3A/Anaconda3/envs/schnetpack/lib/site-packages/schnetpack/data/datamodule.py?line=300'>301</a>\u001b[0m         t\u001b[39m.\u001b[39;49mdatamodule(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/schnetpack/lib/site-packages/schnetpack/data/datamodule.py?line=301'>302</a>\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_transforms:\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/schnetpack/lib/site-packages/schnetpack/data/datamodule.py?line=302'>303</a>\u001b[0m         t\u001b[39m.\u001b[39mdatamodule(\u001b[39mself\u001b[39m)\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\schnetpack\\lib\\site-packages\\schnetpack\\transform\\atomistic.py:91\u001b[0m, in \u001b[0;36mRemoveOffsets.datamodule\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/Anaconda3/envs/schnetpack/lib/site-packages/schnetpack/transform/atomistic.py?line=87'>88</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_datamodule \u001b[39m=\u001b[39m value\n\u001b[0;32m     <a href='file:///d%3A/Anaconda3/envs/schnetpack/lib/site-packages/schnetpack/transform/atomistic.py?line=89'>90</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mremove_atomrefs:\n\u001b[1;32m---> <a href='file:///d%3A/Anaconda3/envs/schnetpack/lib/site-packages/schnetpack/transform/atomistic.py?line=90'>91</a>\u001b[0m     atrefs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_datamodule\u001b[39m.\u001b[39;49mtrain_dataset\u001b[39m.\u001b[39;49matomrefs\n\u001b[0;32m     <a href='file:///d%3A/Anaconda3/envs/schnetpack/lib/site-packages/schnetpack/transform/atomistic.py?line=91'>92</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39matomref \u001b[39m=\u001b[39m atrefs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_property]\u001b[39m.\u001b[39mdetach()\n\u001b[0;32m     <a href='file:///d%3A/Anaconda3/envs/schnetpack/lib/site-packages/schnetpack/transform/atomistic.py?line=93'>94</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mremove_mean:\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\schnetpack\\lib\\site-packages\\schnetpack\\data\\atoms.py:386\u001b[0m, in \u001b[0;36mASEAtomsData.atomrefs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/schnetpack/lib/site-packages/schnetpack/data/atoms.py?line=382'>383</a>\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/schnetpack/lib/site-packages/schnetpack/data/atoms.py?line=383'>384</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39matomrefs\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/schnetpack/lib/site-packages/schnetpack/data/atoms.py?line=384'>385</a>\u001b[0m     md \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata\n\u001b[1;32m--> <a href='file:///d%3A/Anaconda3/envs/schnetpack/lib/site-packages/schnetpack/data/atoms.py?line=385'>386</a>\u001b[0m     arefs \u001b[39m=\u001b[39m md[\u001b[39m\"\u001b[39;49m\u001b[39matomrefs\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/schnetpack/lib/site-packages/schnetpack/data/atoms.py?line=386'>387</a>\u001b[0m     arefs \u001b[39m=\u001b[39m {k: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconversions[k] \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mtensor(v) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m arefs\u001b[39m.\u001b[39mitems()}\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/schnetpack/lib/site-packages/schnetpack/data/atoms.py?line=387'>388</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m arefs\n",
      "\u001b[1;31mKeyError\u001b[0m: 'atomrefs'"
     ]
    }
   ],
   "source": [
    "qm9data = QM9(\n",
    "    './qm9.db', \n",
    "    batch_size=100,\n",
    "    num_train=0.8,\n",
    "    num_val=0.2,\n",
    "    transforms=[\n",
    "        trn.ASENeighborList(cutoff=5.),\n",
    "        # trn.RemoveOffsets(QM9.U0, remove_mean=True, remove_atomrefs=True),\n",
    "        trn.CastTo32()\n",
    "    ],\n",
    "    property_units={QM9.U0: 'eV'},\n",
    "    num_workers=1,\n",
    "    split_file=os.path.join(qm9tut, \"split.npz\"),\n",
    "    pin_memory=True, # set to false, when not using a GPU\n",
    "    load_properties=[QM9.U0], #only load U0 property,\n",
    ")\n",
    "qm9data.prepare_data()\n",
    "qm9data.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is downloaded and partitioned automatically. PyTorch `DataLoader`s can be obtained using `qm9data.train_dataloader()`, `qm9data.val_dataloader()` and `qm9data.test_dataloader()`.\n",
    "\n",
    "Before building the model, we remove offsets from the energy for good initial conditions. We will get this from the training dataset. Above, this is done automatically by the `RemoveOffsets` transform.\n",
    "In the following we show what happens under the hood.\n",
    "For QM9, we also have single-atom reference values stored in the metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U0 of hyrogen: -13.613121032714844 eV\n",
      "U0 of carbon: -1029.863037109375 eV\n",
      "U0 of oxygen: -2042.611083984375 eV\n"
     ]
    }
   ],
   "source": [
    "atomrefs = qm9data.train_dataset.atomrefs\n",
    "print('U0 of hyrogen:', atomrefs[QM9.U0][1].item(), 'eV')\n",
    "print('U0 of carbon:', atomrefs[QM9.U0][6].item(), 'eV')\n",
    "print('U0 of oxygen:', atomrefs[QM9.U0][8].item(), 'eV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These can be used together with the mean and standard deviation of the energy per atom to initialize the model with a good guess of the energy of a molecule. When calculating these statistics, we pass the atomref to take into account, that the model will add these atomrefs to the predicted energy later, so that this part of the energy does not have to be considered in the statistics, i.e.\n",
    "\\begin{equation}\n",
    "\\mu_{U_0} = \\frac{1}{n_\\text{train}} \\sum_{n=1}^{n_\\text{train}} \\left( U_{0,n} - \\sum_{i=1}^{n_{\\text{atoms},n}} U_{0,Z_{n,i}} \\right)\n",
    "\\end{equation}\n",
    "for the mean and analogously for the standard deviation. In this case, this corresponds to the mean and std. dev of the *atomization energy* per atom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean atomization energy / atom: -4.247325399125455\n",
      "Std. dev. atomization energy / atom: 0.18015809859127724\n"
     ]
    }
   ],
   "source": [
    "means, stddevs = qm9data.get_stats(\n",
    "    QM9.U0, divide_by_atoms=True, remove_atomref=True\n",
    ")\n",
    "print('Mean atomization energy / atom:', means.item())\n",
    "print('Std. dev. atomization energy / atom:', stddevs.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the model\n",
    "\n",
    "Next, we need to build the model and define how it should be trained.\n",
    "\n",
    "In SchNetPack, a neural network potential usually consists of three parts:\n",
    "\n",
    "1. A list of input modules that prepare the batched data before the building the representation.\n",
    "   This includes, e.g., the calculation of pairwise distances between atoms based on neighbor indices or add auxiliary\n",
    "   inputs for response properties.\n",
    "2. The representation which either constructs atom-wise features, e.g. with SchNet or PaiNN.\n",
    "3. One or more output modules for property prediction.\n",
    "\n",
    "Here, we use the `SchNet` representation with 3 interaction layers, a 5 Angstrom cosine cutoff with pairwise distances\n",
    "expanded on 20 Gaussians and 50 atomwise features and convolution filters, since we only have a few\n",
    "training examples. Then, we use an `Atomwise` module to predict the inner energy $U_0$ by summing over atom-wise\n",
    "energy contributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 5.\n",
    "n_atom_basis = 30\n",
    "\n",
    "pairwise_distance = spk.atomistic.PairwiseDistances() # calculates pairwise distances between atoms\n",
    "radial_basis = spk.nn.GaussianRBF(n_rbf=20, cutoff=cutoff)\n",
    "schnet = spk.representation.SchNet(\n",
    "    n_atom_basis=n_atom_basis, n_interactions=3,\n",
    "    radial_basis=radial_basis,\n",
    "    cutoff_fn=spk.nn.CosineCutoff(cutoff)\n",
    ")\n",
    "pred_U0 = spk.atomistic.Atomwise(n_in=n_atom_basis, output_key=QM9.U0)\n",
    "\n",
    "nnpot = spk.model.NeuralNetworkPotential(\n",
    "    representation=schnet,\n",
    "    input_modules=[pairwise_distance],\n",
    "    output_modules=[pred_U0],\n",
    "    postprocessors=[trn.CastTo64(), trn.AddOffsets(QM9.U0, add_mean=True, add_atomrefs=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The last argument here is a list of postprocessors that will only be used if `nnpot.inference_mode=True` is set.\n",
    "It will not be used in training or validation, but only for predictions.\n",
    "Here, this is used to deal with numerical accuracy and normalization of model outputs:\n",
    "To make training easier, we have subtracted single atom energies as well as the mean energy per atom\n",
    "in the preprocessing (see above).\n",
    "This does not matter for the loss, but for the final prediction we want to get the real energies.\n",
    "Additionally, we have removed the energy offsets *before* casting to float32 in the preprocessor.\n",
    "This avoids loss of numerical precision.\n",
    "Analog to this, we also have to first cast to float64, before re-adding the offsets in the post-processor\n",
    "\n",
    "The output modules store the prediction in a dictionary under the `output_key` (here: `QM9.U0`), which is connected to\n",
    "a target property with loss functions and evaluation metrics using the `ModelOutput` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "output_U0 = spk.task.ModelOutput(\n",
    "    name=QM9.U0,\n",
    "    loss_fn=torch.nn.MSELoss(),\n",
    "    loss_weight=1.,\n",
    "    metrics={\n",
    "        \"MAE\": torchmetrics.MeanAbsoluteError()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "By default, the target is assumed to have the same name as the output. Otherwise, a different `target_name`\n",
    "has to be provided.\n",
    "Here, we already gave the output the same name as the target in the dataset (`QM9.U0`).\n",
    "In case of multiple outputs, the full loss is a weighted sum of all output losses.\n",
    "Therefore, it is possible to provide a `loss_weight`, which we here just set to 1.\n",
    "\n",
    "All components defined above are then passed to `AtomisticTask`, which is a sublass of\n",
    "[`LightningModule`](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html).\n",
    "This connects the model and training process and can then be passed to the PyTorch Lightning `Trainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "task = spk.task.AtomisticTask(\n",
    "    model=nnpot,\n",
    "    outputs=[output_U0],\n",
    "    optimizer_cls=torch.optim.AdamW,\n",
    "    optimizer_args={\"lr\": 1e-4}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "Now, the model is ready for training. Since we already defined all necessary components, the only thing left to do is\n",
    "passing it to the PyTorch Lightning `Trainer` together with the data module.\n",
    "\n",
    "Additionally, we can provide callbacks that take care of logging, checkpointing etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/kschuett/anaconda3/envs/spkdev/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1582: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\n",
      "  rank_zero_warn(\n",
      "/home/kschuett/anaconda3/envs/spkdev/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:469: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n",
      "\n",
      "  | Name    | Type                   | Params\n",
      "---------------------------------------------------\n",
      "0 | model   | NeuralNetworkPotential | 16.4 K\n",
      "1 | outputs | ModuleList             | 0     \n",
      "---------------------------------------------------\n",
      "16.4 K    Trainable params\n",
      "0         Non-trainable params\n",
      "16.4 K    Total params\n",
      "0.066     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a133ffa729d4437eb3226eba29ca6ace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kschuett/anaconda3/envs/spkdev/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/kschuett/anaconda3/envs/spkdev/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:59: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 100. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n",
      "/home/kschuett/anaconda3/envs/spkdev/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:116: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/kschuett/anaconda3/envs/spkdev/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:412: UserWarning: The number of training samples (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "225afdff1f244d9db148a88f2a78bd51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e14535145f90475a957a97ee9a5f7bd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6874823968d24f998bf9009af215aa24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ce88b1b46d40aebe561072a44e6b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger = pl.loggers.TensorBoardLogger(save_dir=qm9tut)\n",
    "callbacks = [\n",
    "    spk.train.ModelCheckpoint(\n",
    "        inference_path=os.path.join(qm9tut, \"best_inference_model\"),\n",
    "        save_top_k=1,\n",
    "        monitor=\"val_loss\"\n",
    "    )\n",
    "]\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=callbacks,\n",
    "    logger=logger,\n",
    "    default_root_dir=qm9tut,\n",
    "    max_epochs=3, # for testing, we restrict the number of epochs\n",
    ")\n",
    "trainer.fit(task, datamodule=qm9data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ModelCheckpoint` of SchNetPack is equivalent to that in PyTorch Lightning,\n",
    "except that we also store the best inference model. We will show how to use this in the next section.\n",
    "\n",
    "You can have a look at the training log using Tensorboard:\n",
    "```\n",
    "tensorboard --logdir=qm9tut/default\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Having trained a model for QM9, we are going to use it to obtain some predictions.\n",
    "First, we need to load the model. The `Trainer` stores the best model in the model directory which can be loaded using PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from ase import Atoms\n",
    "\n",
    "best_model = torch.load(os.path.join(qm9tut, 'best_inference_model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can use the test dataloader from the QM( data to obtain a batch of molecules and apply the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result dictionary: {'energy_U0': tensor([11774.3995, 10674.7092, 10336.3100, 11225.4174,  9808.5495, 10313.0941,\n",
      "        10737.9138, 10822.9320, 10336.9593, 11225.9045,  8719.2882, 11773.0849,\n",
      "         9763.9021,  9786.9482, 10675.5382, 11837.0301, 12219.8479, 11818.1505,\n",
      "        11689.9811, 10381.7697, 10780.0383, 10292.8703, 10255.7486, 11354.0601,\n",
      "        10372.0447, 11328.8663, 10825.3296, 10380.5685, 11374.1378, 10231.7369,\n",
      "        11774.1743, 12240.5745, 11668.0797, 11331.6547, 12197.7266, 11817.8596,\n",
      "        11795.9724, 10802.3730, 12643.4294, 12198.9642,  9808.1227, 11203.0182,\n",
      "        13805.0691, 10360.3273, 11797.6667, 12431.5858, 11796.2583, 10379.8109,\n",
      "        11370.1223, 10335.8519,  9736.0116, 11225.1377, 11352.5657, 10800.0178,\n",
      "        10802.7024, 11396.5917, 11351.0837, 12831.2083, 12368.0770, 11372.5354,\n",
      "        11248.3640, 11795.8115, 13213.4434,  9387.1987, 10781.3485, 12325.7788,\n",
      "        11668.8270, 10231.1422, 13235.5105,  9319.7437,  5194.6013, 10297.2585,\n",
      "        11670.1327, 11648.3546, 10358.1942,  9387.4924, 12345.5267, 11639.6398,\n",
      "        11224.0048,  9552.0740, 12240.1212, 10518.4759, 11754.8642, 11648.4826,\n",
      "        10328.6224, 11118.9744, 11690.7071, 12197.8641, 11394.3093, 11371.5435,\n",
      "        10358.5896, 10802.8291, 11246.9137, 10335.4797, 11395.6447, 10780.8476,\n",
      "        11775.4573, 12791.9836,  9312.3371, 13322.4199], dtype=torch.float64,\n",
      "       grad_fn=<SubBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "for batch in qm9data.test_dataloader():\n",
    "    result = best_model(batch)\n",
    "    print(\"Result dictionary:\", result)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your data is not already in SchNetPack format, a convenient way is to use ASE atoms with the\n",
    "provided `AtomsConverter` and (optionally) the `SpkCalculator`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "converter = spk.interfaces.AtomsConverter(neighbor_list=trn.ASENeighborList(cutoff=5.), dtype=torch.float32)\n",
    "calculator = spk.interfaces.SpkCalculator(model=best_model, converter=converter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "numbers = np.array([6, 1, 1, 1, 1])\n",
    "positions = np.array([[-0.0126981359, 1.0858041578, 0.0080009958],\n",
    "                      [0.002150416, -0.0060313176, 0.0019761204],\n",
    "                      [1.0117308433, 1.4637511618, 0.0002765748],\n",
    "                      [-0.540815069, 1.4475266138, -0.8766437152],\n",
    "                      [-0.5238136345, 1.4379326443, 0.9063972942]])\n",
    "atoms = Atoms(numbers=numbers, positions=positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: ['_n_atoms', '_atomic_numbers', '_positions', '_cell', '_pbc', '_idx_i_local', '_idx_j_local', '_offsets', '_idx_m', '_idx_j', '_idx_i']\n",
      "Prediction: tensor([1064.6565], dtype=torch.float64, grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "inputs = converter(atoms)\n",
    "\n",
    "print('Keys:', list(inputs.keys()))\n",
    "\n",
    "pred = best_model(inputs)\n",
    "\n",
    "print('Prediction:', pred[QM9.U0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, one can use the `SpkCalculator` as an interface to ASE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 1064.6565392017365\n"
     ]
    }
   ],
   "source": [
    "calculator = spk.interfaces.SpkCalculator(model=best_model, converter=converter, energy=QM9.U0, energy_units=\"eV\")\n",
    "atoms.set_calculator(calculator)\n",
    "print('Prediction:', atoms.get_total_energy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The calculator automatically converts the prediction of the given unit to internal ASE units, which is `eV`\n",
    "for the energy.\n",
    "Using the calculator interface makes more sense if you have trained SchNet for a potential energy surface.\n",
    "In the next tutorials, we will show how to learn potential energy surfaces and forces field as well as performing\n",
    "molecular dynamics simulations with SchNetPack."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e6dc297898d1f04f7bda1814be7ebc514030c95d206cbb809f2abe9474dc68b6"
  },
  "kernelspec": {
   "display_name": "Python [conda env:spkdev] *",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "nbsphinx": {
   "execute": "never"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
